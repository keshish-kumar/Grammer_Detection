{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GRAMMER_PROCESSING.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/keshish-kumar/Grammer_Detection/blob/main/GRAMMER_PROCESSING.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvAC71-IrOuW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "993e2db6-51b7-4c0b-cbfa-e5ac92194688"
      },
      "source": [
        "!pip install spacy\n",
        "import spacy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (2.2.4)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.19.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (4.41.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.8.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy) (51.3.3)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (3.0.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.4.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JciH0KqYspAp"
      },
      "source": [
        "nlp=spacy.load(\"en_core_web_sm\") #English multi-task CNN trained on OntoNotes. Assigns context-specific token vectors, POS tags, dependency parse and named entities."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ax5IQm62s5fn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a21028d-a76e-4ac6-e6ae-a66a4b411071"
      },
      "source": [
        "from google.colab import drive # First u save data set in drive, The colab and drive email id is same \n",
        "drive.mount('/content/gdrive')\n",
        "document =open('gdrive/My Drive/Test.txt').read() # Here i am extracting \n",
        "#the data set into document that is variable form the google drive and u can also upload data directly if u unable then cotect me i can solve it\n",
        "document=nlp(document)\n",
        "dir(document)\n",
        "print(document[0])   # After runing this cell u need authorization code here \n",
        "#comming link click and allow is after allowing code is comming than copy is and paste into box press enter"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n",
            "Hii\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmPtgYjav1TR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c4a02d4-610a-40bc-99d2-4d9ff8b25f39"
      },
      "source": [
        "print(document[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXMTvzGZw6ij",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e41fe0f-55c5-487c-a247-98f6268b3a32"
      },
      "source": [
        "# Tokenizations\n",
        "list(document.sents)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Hii,\n",
              " I am aman kumar,,\n",
              " Welcome to my project, I am heping to this project and clear all the doubt and if any query than contact me,\n",
              " i can solve this query ok.]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgTl6BldxUh0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2196546e-5d82-4b31-c5ae-52fcbfb96bc1"
      },
      "source": [
        "# Get all Tags\n",
        "all_tags={w.pos:w.pos_ for w in document}\n",
        "# Part of speech Tagging\n",
        "# all tags of first sentence of our document\n",
        "for word in list(document.sents) [0]:\n",
        "   print (word,word.tag_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hii UH\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjhE-B9vyOUS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae875b7a-f6c1-4d7d-d49e-a3a62db5fc06"
      },
      "source": [
        "#find out noun\n",
        "[chunk.text for chunk in document.noun_chunks]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I',\n",
              " 'aman kumar',\n",
              " 'my project',\n",
              " 'I',\n",
              " 'this project',\n",
              " 'all the doubt',\n",
              " 'any query',\n",
              " 'me',\n",
              " 'i',\n",
              " 'this query']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6agrpd2yjIc"
      },
      "source": [
        "# visual dependencies\n",
        "# define some paramenters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APj_RrZn0p-y"
      },
      "source": [
        "noisy_pos_tags=[\"PROP\"]\n",
        "min_token_tength=2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7uc1NaB0y4Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0504bd9a-50c0-4d59-8306-2887de9b433b"
      },
      "source": [
        "# Function to check if the token is a noise or not\n",
        "def isNoise(token):\n",
        "  is_noise=False\n",
        "  if token.pos_ in noisy_pos_tags:\n",
        "    is_noise=True\n",
        "  elif token.is_stop==True:\n",
        "    is_noise=True\n",
        "  elif len(token.string)<=min_token_tength:\n",
        "    is_noise=True\n",
        "  return is_noise\n",
        "def cleanup(token,lower =True):\n",
        "  if lower:\n",
        "    token =token.lower()\n",
        "  return token.strip()\n",
        "\n",
        "# top unigrams used in the reviews\n",
        "from collections import Counter\n",
        "cleaned_list=[cleanup(word.string) for word in document if not isNoise(word)]\n",
        "Counter(cleaned_list).most_common(5)\n",
        "\n",
        "# Entity Detection\n",
        "labels=set([w.label_ for w in document.ents])\n",
        "for label in labels:\n",
        "  entities=[cleanup(e.string , lower=False) for e in document.ents if label==e.label_]\n",
        "  entities=list(set(entities))\n",
        "  print(label, entities)\n",
        "\n",
        "  # Displying the tokens\n",
        "print(\"\\n Displying tokens\")\n",
        "for  token in document:\n",
        "  print(token.text,token.tag_, token.head.text,token.dep_)\n",
        "\n",
        "# Displying Noun Phrases\n",
        "print(\"\\n Displying noun phrases\")\n",
        "for np in document.noun_chunks:\n",
        "  print(np.text,np.root.dep_,np.root.head.text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PERSON ['aman kumar']\n",
            "\n",
            " Displying tokens\n",
            "Hii UH Hii ROOT\n",
            "I PRP am nsubj\n",
            "am VBP am ROOT\n",
            "aman NNP kumar compound\n",
            "kumar NNP am attr\n",
            ", , kumar punct\n",
            "Welcome VBP heping advcl\n",
            "to IN Welcome prep\n",
            "my PRP$ project poss\n",
            "project NN to pobj\n",
            ", , heping punct\n",
            "I PRP heping nsubj\n",
            "am VBP heping aux\n",
            "heping JJ heping ROOT\n",
            "to IN heping prep\n",
            "this DT project det\n",
            "project NN to pobj\n",
            "and CC heping cc\n",
            "clear VB heping conj\n",
            "all PDT doubt predet\n",
            "the DT doubt det\n",
            "doubt NN clear dobj\n",
            "and CC doubt cc\n",
            "if IN doubt conj\n",
            "any DT query det\n",
            "query NN if pobj\n",
            "than IN query prep\n",
            "contact VB than pcomp\n",
            "me PRP contact dobj\n",
            "i PRP solve nsubj\n",
            "can MD solve aux\n",
            "solve VB solve ROOT\n",
            "this DT query det\n",
            "query NN solve dobj\n",
            "ok UH solve advmod\n",
            ". . solve punct\n",
            "\n",
            " Displying noun phrases\n",
            "I nsubj am\n",
            "aman kumar attr am\n",
            "my project pobj to\n",
            "I nsubj heping\n",
            "this project pobj to\n",
            "all the doubt dobj clear\n",
            "any query pobj if\n",
            "me dobj contact\n",
            "i nsubj solve\n",
            "this query dobj solve\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unZkbsYVdO9-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6de7cb85-02a8-4317-d14c-14f01b516473"
      },
      "source": [
        "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS as stopwords"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.feature_extraction.stop_words module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.feature_extraction.text. Anything that cannot be imported from sklearn.feature_extraction.text is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQRoi9NpeIuD"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import accuracy_score  #Scikit-learn is probably the most useful library for machine learning in Python.\n",
        "# The sklearn library contains a lot of efficient tools for machine learning and statistical modeling including classification, regression, \n",
        "#clustering and dimensionality reduction.\n",
        "from sklearn.base import TransformerMixin\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.svm import LinearSVC"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5UnlazCepek"
      },
      "source": [
        "import string"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Giz3W0oNerdZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d072f5d5-360c-413b-c33e-8a3d2b518602"
      },
      "source": [
        "punctuations= string.punctuation\n",
        "!python3 -m spacy download en\n",
        "spacy.load('en_core_web_sm')\n",
        "from spacy.lang.en import English\n",
        "parser=English()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (51.3.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.3.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.4.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKGrRPvSii0s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef7cb115-4dbc-4b7b-ccf1-de6df41148d9"
      },
      "source": [
        "# Custom transformer using spacy\n",
        "class predictors(TransformerMixin):\n",
        "  def transform(self,x,**transform_params):\n",
        "    return [clean_text(text) for text in x]\n",
        "  def fit(self,x,y=None,**fit_params):\n",
        "    return self\n",
        "  def get_params(self,deep=True):\n",
        "    return {}\n",
        "# Basic utility function to clen the text\n",
        "def clean_text(text):\n",
        "  return text.strip().lower()\n",
        "def spacy_tokenizer(sentence):\n",
        "  tokens=parser(sentence)\n",
        "  tokens=[tok.lemma_.lower().strip() if tok.lemma_ !=\" -PORN-\" else tok.lower_ for tok in tokens]\n",
        "  tokens=[tok for tok in tokens if(tok not in stopwords and tok not in punctuations)]\n",
        "  return tokens\n",
        "# Create vectorization object to generate feature vector we will use custom spacy tokenization\n",
        "vectorizer=CountVectorizer(tokenizer=spacy_tokenizer, ngram_range=(1,1))\n",
        "classifier=LinearSVC()\n",
        "# Creaye the pipeline to clean,tokenize,vectrizer and classify\n",
        "\n",
        "pipe=Pipeline([(\"cleaner\",predictors()),\n",
        "               ('vectorizer', vectorizer),\n",
        "               ('classifier', classifier)])\n",
        "\n",
        "# Load simple data \n",
        "# Here i am just trying to train some handwritting data you can load data set from opensource\n",
        "\n",
        "train=[('I am with RIO Projects.','pos'),\n",
        "       ('this is an amazing place!','pos'),\n",
        "       ('I feel vrey good about these project','pos'),\n",
        "       ('This is my best project','pos'),\n",
        "       ('What an awesome project','pos'),\n",
        "       ('y mvlkb mkjgi ffo','neg'),\n",
        "        ('I am trying to lean this project ','pos'),\n",
        "       ('I am tried of learning','pos'),\n",
        "       ('I can not deal with this','neg'),\n",
        "       ('He is my enemy','neg'),\n",
        "       ('my boss is horrible','pos')]\n",
        "test=[('the project was good ','pos'),   # Here u can change sentence and check accurcy\n",
        "      ('I do not enjoy my job','pos'),\n",
        "      ('i am not fell good today','pos'),\n",
        "      ('He is a good friend of mine','pos'),\n",
        "      (\"I can't believe i'm doing this\",'pos')]\n",
        "\n",
        "# Create model and measure accuracy\n",
        "\n",
        "pipe.fit([x[0] for x in train], [x[1] for x in train])\n",
        "pred_data=pipe.predict([x[0] for x in test])\n",
        "for (sample,pred) in zip(test,pred_data):\n",
        "  print(sample,pred)\n",
        "print(\"accuracy\",accuracy_score([x[1] for x in test], pred_data))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('the project was good ', 'pos') pos\n",
            "('I do not enjoy my job', 'pos') pos\n",
            "('i am not fell good today', 'pos') pos\n",
            "('He is a good friend of mine', 'pos') pos\n",
            "(\"I can't believe i'm doing this\", 'pos') pos\n",
            "accuracy 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9shjS0ACOan"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}